<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js navy">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Garuda&#x27;s infra documentation</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="Contains the documentation for the Garuda Linux infrastructure">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "navy";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('navy')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="intro.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="general.html"><strong aria-hidden="true">2.</strong> General information</a></li><li class="chapter-item expanded "><a href="common.html"><strong aria-hidden="true">3.</strong> Common tasks</a></li><li class="chapter-item expanded "><a href="hosts.html"><strong aria-hidden="true">4.</strong> Hosts</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="hosts/immortalis.html"><strong aria-hidden="true">4.1.</strong> immortalis</a></li><li class="chapter-item expanded "><a href="hosts/garuda-mail.html"><strong aria-hidden="true">4.2.</strong> garuda-mail</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Garuda&#x27;s infra documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/garuda-linux/infrastructure-nix" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-code-fork"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="garuda-linux-server-configurations"><a class="header" href="#garuda-linux-server-configurations">Garuda Linux server configurations</a></h1>
<p><a href="https://builtwithnix.org"><img src="https://img.shields.io/static/v1?logo=nixos&amp;logoColor=white&amp;label=&amp;message=Built%20with%20Nix&amp;color=41439a" alt="built with nix" /></a> <a href="https://github.com/garuda-linux/infrastructure-nix/actions/workflows/flake_check.yml"><img src="https://github.com/garuda-linux/infrastructure-nix/actions/workflows/flake_check.yml/badge.svg?branch=main" alt="run nix flake check" /></a> <a href="https://github.com/garuda-linux/infrastructure-nix/actions/workflows/pages.yml"><img src="https://github.com/garuda-linux/infrastructure-nix/actions/workflows/pages.yml/badge.svg" alt="deploy docs" /></a></p>
<h2 id="general-information"><a class="header" href="#general-information">General information</a></h2>
<ul>
<li>Our current infrastructure is hosted in one of <a href="https://www.hetzner.com/dedicated-rootserver/ax102">these</a>.</li>
<li>The only other server not being contained in this dedicated server is our mail server.</li>
<li>Both servers are being backed up to Hetzner storage boxes via <a href="https://www.borgbackup.org/">Borg</a>.</li>
<li>After multiple different setups, we settled on <a href="https://nixos.org/">NixOS</a> as our main OS as it provides reproducible and atomically updated system states</li>
<li>Most (sub)domains are protected by Cloudflare while also making use of its caching feature. Exemptions are services such as our mail server and parts violating Cloudflares rules such as proxying Piped content.</li>
</ul>
<h2 id="quick-links"><a class="header" href="#quick-links">Quick links</a></h2>
<ul>
<li><a href="./hosts/common.html">Common maintenance tasks</a></li>
<li><a href="./hosts/garuda-mail.html">Host: garuda-mail</a></li>
<li><a href="./hosts/immortalis.html">Host: immortalis</a></li>
</ul>
<h2 id="devshell-and-how-to-enter-it"><a class="header" href="#devshell-and-how-to-enter-it">Devshell and how to enter it</a></h2>
<p>This NixOS flake provides a <a href="https://github.com/numtide/devshell">devshell</a> which contains all deployment tools as well as handy aliases for common tasks.
The only requirement for using it is having the Nix package manager available. It can be installed on various distributions via the package manager or the following script (<a href="https://zero-to-nix.com/start/install">click me for more information</a>):</p>
<pre><code class="language-sh">curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix -o nix-install.sh # Check its content afterwards
sh ./nix-install.sh install --diagnostic-endpoint=&quot;&quot;
</code></pre>
<p>This installs the Nix packages with flakes already pre-enabled. After that, the shell can be invoked as follows:</p>
<pre><code class="language-sh">nix develop # The intended way to use the devshell
nix-shell # Legacy, non-flakes way if flakes are not available for some reason
</code></pre>
<p>This also sets up pre-commit-hooks and shows the currently implemented tasks, which can be executed by running the command.</p>
<pre><code class="language-sh">[infra-nix]

ansible-core    - Radically simple IT automation
apply           - Applies the infra-nix configuration pushed to the servers
buildiso-local  - Spawns a local buildiso shell to build to ./buildiso (needs Docker)
buildiso-remote - Spawn a buildiso shell on the iso-runner builder
clean           - Runs the garbage collection on the servers
deploy          - Deploys the local NixOS configuration to the servers
update          - Performs a full system update on the servers by bumping flake lock
update-forum    - Updates the Discourse container of our forum
update-toolbox  - Updates the locked Chaotic toolbox commit and deploys the changes
update-website  - Updates the locked website commit and deploys the changes
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="general-structure"><a class="header" href="#general-structure">General structure</a></h2>
<p>A general overview of the folder structure can be found below:</p>
<pre><code class="language-sh">├── assets
├── docker-compose
│   ├── all-in-one
│   ├── github-runner
│   └── proxied
├── docs
│   ├── hosts
│   └── theme
├── home-manager
├── host_vars
│   ├── garuda-build
│   ├── garuda-mail
│   └── immortalis
├── nixos
│   ├── hosts
│   │   ├── garuda-build
│   │   ├── garuda-mail
│   │   └── immortalis
│   ├── modules
│   │   └── static
│   └── services
│       ├── chaotic
│       ├── docker-compose-runner
│       └── monitoring
├── playbooks
├── scripts
└── secrets
</code></pre>
<h2 id="secrets"><a class="header" href="#secrets">Secrets</a></h2>
<p>Secrets are managed via a custom Git submodule that contains <code>ansible-vault</code> encrypted files as well as a custom NixOS module <code>garuda-lib</code> which makes them available to our services. The submodule is available in the <code>secrets</code> directory. To view or edit any of these files, one can use the following commands:</p>
<pre><code class="language-sh">ansible-vault decrypt secrets/pathtofile
ansible-vault edit secrets/pathtofile
ansible-vault encrypt secrets/pathtofile
</code></pre>
<p>Further information on <code>ansible-vault</code> can be found in its <a href="https://docs.ansible.com/ansible/latest/vault_guide/index.html">documentation</a>.
It is important to keep the <code>secrets</code> directory in the latest state before deploying a new configuration as misconfigurations might happen otherwise.</p>
<h2 id="linting-and-formatting"><a class="header" href="#linting-and-formatting">Linting and formatting</a></h2>
<p>We utilize <a href="https://github.com/cachix/pre-commit-hooks.nix">pre-commit-hooks</a> to automatically set up the pre-commit-hook with all the tools once <code>nix-shell</code> or <code>nix develop</code> is run for the first time. Checks can then be executed by running one of the following configs:</p>
<pre><code class="language-sh">nix flake check # checks flake outputs and runs pre-commit at the end
pre-commit run --all-files # only runs the pre-commit tools on all files
</code></pre>
<p>Its configuration can be found in the <code>flake.nix</code> file. (<a href="https://gitlab.com/garuda-linux/infra-nix/-/blob/main/flake.nix">click me</a>). At the time of writing, the following tools are being run:</p>
<ul>
<li><a href="https://github.com/rhysd/actionlint">actionlint</a></li>
<li><a href="https://github.com/ansible/ansible-lint">ansible-lint</a></li>
<li><a href="https://github.com/commitizen-tools/commitizen">commitizen</a></li>
<li><a href="https://github.com/astro/deadnix">deadnix</a></li>
<li><a href="https://github.com/oxalica/nil">nil</a></li>
<li><a href="https://github.com/nix-community/nixpkgs-fmt">nixpkgs-fmt</a></li>
<li><a href="https://prettier.io/">prettier</a></li>
<li><a href="https://github.com/nerdypepper/statix">statix</a></li>
<li><a href="https://github.com/adrienverge/yamllint">yamllint</a></li>
</ul>
<p>It is recommended to run <code>pre-commit run --all-files</code> before committing any files. Then use <code>cz commit</code> to generate a <code>commitizen</code> complying commit message.</p>
<h2 id="cicd"><a class="header" href="#cicd">CI/CD</a></h2>
<p>We have used pull-/push-based mirroring for this git repository. This allows easy access to Renovate without having to run a custom instance mirroring changes to both GitHub and GitLab. The following tasks have been automated as of now:</p>
<ul>
<li><code>nix flake check</code> runs for every labeled PR and commit on main.</li>
<li><a href="https://renovatebot.com/">Renovate</a> periodically checks <code>docker-compose.yml</code> and other supported files for version updates. It has a <a href="https://github.com/garuda-linux/infrastructure-nix/issues/5">dependency dashboard</a> as well as the <a href="https://developer.mend.io/github/garuda-linux/infrastructure-nix">developer interface</a> to check logs of individual runs. Minor updates appear as grouped PRs while major updates are separated from those. Note that this only applies to the GitHub side.</li>
<li>Deployment of our mdBook-based documentation to Cloudflare pages.</li>
</ul>
<h2 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h2>
<p>Our current monitoring stack mostly relies on Netdata to provide insight into current system loads and trends. The major reason for using it was that it provides the most vital metrics and alerts out of the box without having to create in-depth configurations. Might switch to the Prometheus/Grafana/Loki stack in the future. We used to set up children -&gt; parent streaming in the past, though after transitioning to one big host this didn't make sense anymore. Instead, up to 10GB of data gets stored on individual hosts. While Netdata agents do have their dashboard, the <a href="https://app.netdata.cloud/spaces/garuda-infra/rooms/all-nodes">Dashboard provided by Netdata</a> is far superior and allows a better insight, eg. by offering the functions feature. Additional services like Squid or Nginx have been configured to be monitored by Netdata plugins as well. Further information can be found in its <a href="https://learn.netdata.cloud/">documentation</a>. To access the previously linked dashboard, use <code>team@garudalinux.org</code> as login, the login will be completed after opening the link sent here.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="common-maintenance-tasks"><a class="header" href="#common-maintenance-tasks">Common maintenance tasks</a></h2>
<h3 id="rebuilding--updating-the-forum-container"><a class="header" href="#rebuilding--updating-the-forum-container">Rebuilding / updating the forum container</a></h3>
<p>Sometimes Discourse needs its container to build rebuild via cli rather than the webinterface. This can be done with:</p>
<pre><code class="language-sh">ssh -p 224 $user@116.202.208.112
cd /var/discourse
sudo ./launcher rebuild app
</code></pre>
<h3 id="building-iso-files"><a class="header" href="#building-iso-files">Building ISO files</a></h3>
<p>To build Garuda ISO, one needs to connect to the <code>iso-runner</code> container and execute the <code>buildiso</code> command, which opens a shell containing the needed environment:</p>
<pre><code class="language-sh">ssh -p 227 $user@116.202.208.112 # if one ran nix develop before, this can be skipped
buildiso
buildiso -i # updates the iso-profiles repo
buildiso -p dr460nized
</code></pre>
<p>Further information on available commands can be found in the <a href="https://gitlab.com/garuda-linux/tools/garuda-tools">garuda-tools</a> repository.
After the build process is finished, builds can be found on <a href="https://iso.builds.garudalinux.org/iso/garuda/">iso.builds.garudalinux.org</a> - no automatic pushing to Sourceforge and Cloudflare R2 happens by default, see below for more information on how to achieve this.</p>
<h3 id="deploying-a-new-iso-release"><a class="header" href="#deploying-a-new-iso-release">Deploying a new ISO release</a></h3>
<p>We are assuming all ISOs have been tested for functionality before executing any of those commands.</p>
<pre><code class="language-sh">ssh -p 227 $user@116.202.208.112
build all # builds all ISO provided in the buildall command
deployiso -FS # sync to Cloudflare R2 and Sourceforge
deployiso -FSR # sync to Cloudflare R2 and Sourceforge while also updating the latest (stable, non-nightly) release
deployiso -Sd # to delete the old ISOs on Sourceforge once they aren't needed anymore
deployiso -FSRd # oneliner for the above-given commands
</code></pre>
<h3 id="updating-the-system"><a class="header" href="#updating-the-system">Updating the system</a></h3>
<p>One needs to have the <a href="https://gitlab.com/garuda-linux/infra-nix">infra-nix</a> repo cloned locally. Then proceed by updating the <code>flake.lock</code> file, pushing it to the server &amp; building the configurations:</p>
<pre><code class="language-sh">nix flake update
ansible-playbook garuda.yml -l $servername # Eg. immortalis for the Hetzner host
deploy # Skip using the above command and use this one in case nix develop was used
</code></pre>
<p>Then you can either apply it via Ansible or connect to the host to view more details about the process while it runs:</p>
<pre><code class="language-sh">ansible-playbook apply.yml -l $servername # Ansible

apply # Nix develop shell

ssh -p 666 $user@116.202.208.112 # Manually, exemplary on immortalis
sudo nixos-rebuild switch
</code></pre>
<p>Keep in mind that this will restart every service whose files changed since the last system update. On our Hetzner server, this includes a restart of every declarative <code>nixos-container</code> if needed, causing a small downtime.</p>
<h3 id="changing-system-configurations"><a class="header" href="#changing-system-configurations">Changing system configurations</a></h3>
<p>Most system configurations are contained in individual Nix files in the <code>nix</code> directory of this repo. This means changing anything must not be done manually but by editing the corresponding file and pushing/applying the configuration afterward.</p>
<pre><code class="language-sh">ansible-playbook garuda.yml -l $servername # Eg. immortalis for the Hetzner host
deploy # In case nix develop is used
</code></pre>
<p>As with the system update, one can either apply via Ansible or manually:</p>
<pre><code class="language-sh">ansible-playbook apply.yml -l $servername # Ansible

apply # Nix develop shell

ssh -p 666 $user@116.202.208.112 # Manually, exemplary on immortalis
sudo nixos-rebuild switch
</code></pre>
<h4 id="adding-a-user"><a class="header" href="#adding-a-user">Adding a user</a></h4>
<p>Adding users needs to be done in <code>users.nix</code>:</p>
<ul>
<li>Add a new user <a href="https://gitlab.com/garuda-linux/infra-nix/-/blob/main/nixos/modules/users.nix?ref_type=heads#L14">here</a></li>
<li>Add the SSH public key to <a href="https://gitlab.com/garuda-linux/infra-nix/-/blob/main/flake.nix?ref_type=heads#L43">flake inputs</a></li>
<li>Add the specialArgs <code>keys.user</code> as seen <a href="https://gitlab.com/garuda-linux/infra-nix/-/blob/main/nixos/flake-module.nix?ref_type=heads#L38">here</a></li>
<li>Deploy &amp; apply the configuration</li>
</ul>
<h3 id="changing-docker-configurations"><a class="header" href="#changing-docker-configurations">Changing Docker configurations</a></h3>
<p>If configurations of services running in Docker containers need to be altered, one needs to edit the corresponding <code>docker-compose.yml</code> (<code>./nix/docker-compose/$name</code>) file or <code>.env</code> file in the <code>secrets</code> directory (see the secrets section for details on that topic). The deployment is done the same way as with normal system configuration.</p>
<h3 id="updating-docker-containers"><a class="header" href="#updating-docker-containers">Updating Docker containers</a></h3>
<p>Docker containers sometimes use the <code>latest</code> tag in case no current tag is available or in the case of services like Piped and Searx, where it is often crucial to have the latest build to bypass Google's restrictions. Containers using the <code>latest</code> tag are automatically updated via <a href="https://containrrr.dev/watchtower/">watchtower</a> daily. The remaining ones can be updated by changing their version in the corresponding <code>docker-compose.yml</code> and then running <code>deploy</code> &amp; <code>apply</code>. If containers are to be updated manually, this can be achieved by connecting to the host, running <code>nixos-container root-login $containername</code>, and executing:</p>
<pre><code class="language-sh">cd /var/garuda/docker-compose-runner/$name/ # replace $name with the actual docker-compose.yml or autocomplete via tab
sudo docker compose pull
sudo docker compose up -d
</code></pre>
<p>The updated containers will be pulled and automatically recreated using the new images.</p>
<h3 id="rotating-ipv6"><a class="header" href="#rotating-ipv6">Rotating IPv6</a></h3>
<p>Sometimes it is needed to rotate the available IPv6 addresses to solve the current ones being rate-limited for outgoing requests of Piped, Searx, etc. This can be achieved by editing the hosts Nix file <code>immortalis.nix</code>, replacing the existing values of the <code>networking.interfaces.&quot;eth0&quot;.ipv6.addresses</code> keys seen <a href="https://gitlab.com/garuda-linux/infra-nix/-/blob/main/nixos/hosts/immortalis.nix?ref_type=heads#L30">here</a>. Then, proceed doing the same with the <a href="https://gitlab.com/garuda-linux/infra-nix/-/blob/main/nixos/hosts/immortalis.nix?ref_type=heads#L219">squid configuration</a>. Possible IPv6 addresses need to be generated from our available /64 subnet space and can't be chosen completely random.</p>
<h3 id="checking-whether-backups-were-successful"><a class="header" href="#checking-whether-backups-were-successful">Checking whether backups were successful</a></h3>
<p>To check whether backups to Hetzner are still working as expected, connect to the server and execute the following:</p>
<pre><code class="language-sh">systemctl status borgbackup-job-backupToHetzner
</code></pre>
<p>This should yield a successful unit state. The only exception is having an exit code != <code>0</code> due to files having changed during the run.</p>
<h3 id="updating-the-website-content-or-chaotic-aur-toolbox"><a class="header" href="#updating-the-website-content-or-chaotic-aur-toolbox">Updating the website content or Chaotic-AUR toolbox</a></h3>
<p>This needs to be done by updating the flake input (git repo URL of the website) <a href="https://gitlab.com/garuda-linux/infra-nix/-/blob/main/nix/flake.nix?ref_type=heads#L60">src-garuda-website</a> or <a href="https://gitlab.com/garuda-linux/infra-nix/-/blob/main/nix/flake.nix?ref_type=heads#L44">src-chaotic-toolbox</a>:</p>
<pre><code class="language-sh">cd nix
nix flake lock --update-input src-garuda-website # website
nix flake lock --update-input src-chaotic-toolbox # toolbox
</code></pre>
<p>After that deploy as usual by running <code>deploy</code> and <code>apply</code>. The commit and corresponding hash will be updated and NixOS will use it to build the website or toolbox using the new revision automatically.</p>
<h3 id="updating-the-garuda-startpage-content"><a class="header" href="#updating-the-garuda-startpage-content">Updating the Garuda startpage content</a></h3>
<p>Our startpage consists of a simple <a href="https://github.com/bastienwirtz/homer">homer</a> deployment. Its configuration is stored in the <a href="https://gitlab.com/garuda-linux/website/startpage">startpage</a> repo, which gets cloned to the docker-compose.yml's directory to serve the files. In order, updating is currently done manually after pushing the changes to the repo (might automate this soon via systemd timer!):</p>
<pre><code class="language-sh">ssh -p 225 $user@116.202.208.112
cd /var/garuda/docker-compose-runner/all-in-one/startpage
git pull
sudo docker restart homer
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hosts"><a class="header" href="#hosts">Hosts</a></h1>
<p>These are our currently available servers:</p>
<ul>
<li><a href="./hosts/immortalis.html">immortalis</a></li>
<li><a href="./hosts/garuda-mail.html">garuda-mail</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="immortalis-hetzner-dedicated"><a class="header" href="#immortalis-hetzner-dedicated">immortalis (Hetzner dedicated)</a></h2>
<h3 id="general"><a class="header" href="#general">General</a></h3>
<p>This system utilizes a NixOS host which uses <a href="https://nixos.wiki/wiki/NixOS_Containers">nixos-containers</a> to build declarative <code>systemd-nspawn</code> machines for different purposes. To make the best use of the available resources, common directories are shared between containers. This includes <code>/home</code> (home-manager / NixOS configurations writing to home are generated by the host and disabled for the containers), Pacman and Chaotic cache, the <code>/nix</code> directory, and a few others. Further details can be found in the <a href="hhttps://gitlab.com/garuda-linux/infra-nix/-/blob/main/nixos/hosts/immortalis/containers.nix">Nix expression</a> of the host.</p>
<p>All directories containing important data were mapped to <code>/data_1</code> and <code>/data_2</code> to have them all in one place. The first mostly contains web services' files, the latter only builds related directories such as the Pacman cache.</p>
<p>The current line-up looks as follows:</p>
<pre><code>nico@immortalis ~ (main)&gt; machinectl
MACHINE        CLASS     SERVICE        OS    VERSION ADDRESSES
chaotic-kde    container systemd-nspawn nixos 23.11   10.0.5.90
docker         container systemd-nspawn nixos 23.11   10.0.5.100
docker-proxied container systemd-nspawn nixos 23.11   10.0.5.110
forum          container systemd-nspawn nixos 23.11   10.0.5.70
github-runner  container systemd-nspawn nixos 23.11   10.0.5.130
iso-runner     container systemd-nspawn nixos 23.11   10.0.5.40
lemmy          container systemd-nspawn nixos 23.11   10.0.5.120
mastodon       container systemd-nspawn nixos 23.11   10.0.5.80
meshcentral    container systemd-nspawn nixos 23.11   10.0.5.60
postgres       container systemd-nspawn nixos 23.11   10.0.5.50
repo           container systemd-nspawn nixos 23.11   10.0.5.30
temeraire      container systemd-nspawn nixos 23.11   10.0.5.20
web-front      container systemd-nspawn nixos 23.11   10.0.5.10
</code></pre>
<p>We are seeing:</p>
<ul>
<li>1 ISO builder (<code>iso-runner</code>)</li>
<li>1 reverse proxy serving all the websites and services (<code>web-front</code>)</li>
<li>2 Docker dedicated nspawn containers (<code>docker</code> &amp; <code>docker-proxied</code>)</li>
<li>4 Chaotic-AUR builders (<code>chaotic-kde</code>, <code>github-runner</code>, <code>repo</code> &amp; <code>temeraire</code>)</li>
<li>5 app dedicated containers (<code>forum</code>, <code>lemmy</code>, <code>mastodon</code>, <code>meshcentral</code> &amp; <code>postgres</code>)</li>
</ul>
<h3 id="connecting-to-the-server"><a class="header" href="#connecting-to-the-server">Connecting to the server</a></h3>
<p>After connecting to the host via <code>ssh -p 666 $user@116.202.208.112</code>, containers can generally be entered by running <code>nixos-container login $containername</code>, eg. <code>nixos-container login web-front</code>. Some containers may also be connected via SSH using the following ports:</p>
<ul>
<li>22: <code>temeraire</code> (needs to be 22 to allow pushing packages to the main Chaotic-AUR node via rsync)</li>
<li>223: <code>repo</code></li>
<li>224: <code>forum</code></li>
<li>225: <code>docker</code></li>
<li>226: <code>chaotic-kde</code></li>
<li>227: <code>iso-runner</code></li>
<li>228: <code>web-front</code></li>
<li>229: <code>postgres</code> (access the database in <code>127.0.0.1</code> via <code>ssh -p 229 nico@116.202.208.112 -L 5432:127.0.0.1:5432</code>)</li>
</ul>
<h3 id="docker-containers"><a class="header" href="#docker-containers">Docker containers</a></h3>
<p>Some services not packaged in NixOS or are easier to deploy this way are serviced via the Docker engine. This contains services like Piped, Whoogle, and Matrix. We use a custom <a href="https://gitlab.com/garuda-linux/infra-nix/-/blob/main/nix/garuda/services/docker-compose-runner/docker-compose-runner.nix?ref_type=heads">NixOS module</a> to deploy those with the rest of the system. Secrets are handled via our secret management which consists of a git submodule <code>secret</code> (private repo with <code>ansible-vault</code> encrypted files) and <code>garuda-lib</code> (see secrets section). Those contain a <code>docker-compose</code> directory in which the <code>.env</code> files for the <code>docker-compose.yml</code> are stored.</p>
<h3 id="chaotic-aur--repository"><a class="header" href="#chaotic-aur--repository">Chaotic-AUR / repository</a></h3>
<p>Our repository leverages <a href="https://aur.chaotic.cx">Chaotic-AUR's</a> <a href="https://github.com/chaotic-aur/toolbox">toolbox</a> to provide the main node for the <code>[chaotic-aur]</code> repository as well as two more instances building the <code>[garuda]</code> and <code>[chaotic-kde]</code> repositories. Users of the <code>chaotic_op</code> group may build packages on the corresponding nixos-container via the <a href="https://github.com/chaotic-aur/toolbox/blob/main/README.md">chaotic</a> command:</p>
<pre><code>chaotic get $package # pull PKGBUILD
chaotic mkd $package # build package in the previously cloned directory
chaotic bump $package # increment pkgver of $package by 0.1 to allow a rebuild
chaotic rm $package # remove the package from the repository
</code></pre>
<p>Further information may be obtained by clicking <code>chaotic</code> seen above. The corresponding builders are:</p>
<ul>
<li><code>[chaotic-aur]</code>: <code>temeraire</code></li>
<li><code>[garuda]</code>: <code>repo</code></li>
<li><code>[chaotic-kde]</code>: <code>chaotic-kde</code></li>
</ul>
<h3 id="squid-proxy"><a class="header" href="#squid-proxy">Squid proxy</a></h3>
<p>Squid is being installed on the host machine to proxy outgoing requests via random IPv6 addresses of the /64 subnet Hetzner provides for services that need it, eg. Piped, the Chaotic-AUR builders, and other services that are getting rate limited quickly. The process is not entirely automated, which means that we currently have a pool of IPv6 addresses active and need to switch them whenever those are getting rate-limited again.
Since we supplied an invalid IPv4 to force outgoing IPv6, the log files were somewhat cluttered by (expected) errors. Systemd-unit logging has been set to <code>LogLevelMax=1</code> to un-clutter the journal and needs to be increased again if debugging needs to be done.</p>
<h3 id="backups"><a class="header" href="#backups">Backups</a></h3>
<p>Backups are provided by daily Borg runs. Only the <code>/data_1</code> directory is backed up (minus <code>/data_1/{dockercache,dockerdata}</code>) as the rest are either Nix-generated or build-related files that can easily recovered from another repository mirror. The corresponding systemd-unit is named <code>borgbackup-job-backupToHetzner</code>.</p>
<h3 id="tailscale--mesh-network"><a class="header" href="#tailscale--mesh-network">Tailscale / mesh network</a></h3>
<p>While Tailscale was commonly used to connect multiple VMs before, this server only has it active on the host. However, we are leveraging Tailscale's <a href="https://tailscale.com/kb/1019/subnets/">subnet router</a> feature to serve the <code>10.0.5.0/24</code> subnet via Tailscale, which means that other Tailscale clients may access the <code>nixos-containers</code> via their IP if <code>tailscale up --accept-routes</code> was used to set up the service.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="garuda-mail-netcup-vps"><a class="header" href="#garuda-mail-netcup-vps">garuda-mail (Netcup VPS)</a></h2>
<h3 id="general-1"><a class="header" href="#general-1">General</a></h3>
<p>This system mainly consists of the <a href="https://gitlab.com/simple-nixos-mailserver/nixos-mailserver">simple-nixos-mailserver</a>. Its only purpose is providing a mail service to team members. The current config looks like <a href="https://gitlab.com/garuda-linux/infra-nix/-/blob/main/nixos/hosts/garuda-mail.nix?ref_type=heads#L47">this</a>.
In case of issues, the <a href="https://nixos-mailserver.readthedocs.io/en/latest/">documentation</a> can be consulted.</p>
<h3 id="mail-server-setup"><a class="header" href="#mail-server-setup">Mail server setup</a></h3>
<p>The mail server details are as follows:</p>
<ul>
<li>host: <code>mail.garudalinux.net</code></li>
<li>incoming: IMAP via <code>993</code></li>
<li>outgoing: SMTP via <code>587/465</code></li>
</ul>
<h3 id="backups-1"><a class="header" href="#backups-1">Backups</a></h3>
<p>Backups are happening daily via Borg. A Hetzner storage box is used to store multiple generations of backups.</p>
<h3 id="creating-a-new-user"><a class="header" href="#creating-a-new-user">Creating a new user</a></h3>
<p>A new user can be created be adding a new <code>loginAccounts</code> value and supplying the password via <code>secrets</code>. We make use of <code>hashedPasswordFile</code>, therefore new hashes can be generated by running <code>nix-shell -p mkpasswd --run 'mkpasswd -sm bcrypt'</code>. Add it to the <code>secrets</code>, <code>deploy</code> and <code>apply</code>. Don't forget to commit both changes.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
